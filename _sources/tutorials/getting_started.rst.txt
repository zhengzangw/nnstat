.. _tutorials.getting_started:

Getting Started
===================

``NNstat`` is a library for analyzing the status and statistics of neural networks. The basic element for NNstat is ``StateDict``. ``StateDict`` is a ``dict``-like object, the keys are the names of the parameters. The values can be ``torch.Tensor`` including weights, gradients, optimizer states, and activations. The values can also be ``float`` describing the statistics of the parameters.

Get the Params
-------------------

With ``NNstat``, it is easy to get the information of a neural network. For example, we can get the weights of a randomly initialized ResNet18 model via :meth:`from_weight() <nnstat.core.StateDict.from_weight>`.

>>> import nnstat
>>> from torchvision import models
>>> state_dict = nnstat.from_state_dict(models.resnet18().state_dict())
# or in a recommended way
>>> state_dict = nnstat.from_weight(models.resnet18())
>>> print(state_dict)
ResNet_weights[L2=113.1, Numel=11,689,512, device=cpu, dtype=None]
(
        00: conv1.weight                    (64, 3, 7, 7)
        01: bn1.weight                              (64,)
        02: bn1.bias                                (64,)
        03: layer1.0.conv1.weight          (64, 64, 3, 3)
        04: layer1.0.bn1.weight                     (64,)
        05: layer1.0.bn1.bias                       (64,)
        (...truncated)
)

To get detailed statistics for each parameter, we can use :meth:`describe <nnstat.core.StateDict.describe>` method.

>>> state_dict.describe() # This prints the statistics of all parameters in the state_dict.
[============================= Stats ResNet_weights ==============================]
   no name            shape         numel     norm1          norm1_mean  norm2   
0  62  ResNet_weights  (11689512,)  11689512  235797.859375  0.020172    113.1194
>>> state_dict.describe(lw=True, pattern='.*conv.*') # This prints the statistics of all conv layers layerwisely.
[=================================== Stats ResNet_weights ====================================]
    no name                   shape              numel    norm1         norm1_mean  norm2    
0    0  conv1.weight              (64, 3, 7, 7)     9408    188.483765  0.020034     2.434446
1    3  layer1.0.conv1.weight    (64, 64, 3, 3)    36864   1720.965454  0.046684    11.248564
2    6  layer1.0.conv2.weight    (64, 64, 3, 3)    36864   1737.153442  0.047123    11.347944
3    9  layer1.1.conv1.weight    (64, 64, 3, 3)    36864   1725.072632  0.046796    11.237497
(...truncated)

The ``lw`` argument means layerwise statistics. The ``pattern`` argument is a regular expression to filter the parameters. We can customize the statistics as follows.

>>> state_dict.describe(lw=True, pattern='.*conv.*', include_keys=['name', 'shape', 'numel', 'mean', 'std', 'norm2'])
[============================== Stats ResNet_weights ===============================]
   name                   shape              numel    mean      std       norm2    
0   conv1.weight              (64, 3, 7, 7)     9408 -0.000200  0.025099   2.434446
1   layer1.0.conv1.weight    (64, 64, 3, 3)    36864  0.000027  0.058587  11.248564
2   layer1.0.conv2.weight    (64, 64, 3, 3)    36864  0.000487  0.059103  11.347944
(...truncated)

Let's do a quick check. For ``conv1.weight``, the mean is close to zero, and the standard deviation is determined by `Kaiming initialization <https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html>`_. With a fanout of :math:`o=64\times 7\times 7=3136`, the std is :math:`\sigma=\sqrt{2/o}=0.0253\approx 0.0251`. Then the L2 norm is :math:`||w||_2=\sqrt{N*\sigma^2}=\sqrt{N}\sigma=2.436\approx 2.434`.


Get the Update
-------------------

Now let's turn to another example. We want to get the update of a GPT-2 model. We train a GPT-2-small with `nanoGPT-Sophia <https://github.com/Liuhong99/Sophia>`_ and use the checkpoint at the 50000 step. The first way to compute the update is calculating the difference between the current state and the previous state.

.. code-block:: python
    :emphasize-lines: 1,6,7

    weight_0 = nnstat.from_weight(model, clone=True)
    # train loops start here
        # model forward
        loss.backward()
        optimizer.step()
        update = nnstat.from_weight(model) - weight_0
        weight_0 = nnstat.from_weight(model, clone=True)

Note that without ``clone=True``, ``weight_0`` will be updated with the model. Thus, an equivalent way is:

.. code-block:: python
    :emphasize-lines: 1,2,7,8

    weight = nnstat.from_weight(model)
    weight_0 = weight.clone()
    # train loops start here
        # model forward
        loss.backward()
        optimizer.step()
        update = weight - weight_0
        weight_0 = weight.clone()

.. caution::
    For weights (:meth:`from_weight <nnstat.core.from_weight>`) and optimizer states (:meth:`from_optimizer_state <nnstat.core.from_optimizer_state>`), ``clone=False`` keeps track of the changes. For gradients (:meth:`from_grad <nnstat.core.from_grad>`) and activations (:meth:`from_activation <nnstat.core.from_activation>`), ``clone=False`` will not keep track of the changes because the gradients and activations will be cleared after each iteration.

Since the optimizer is :class:`AdamW <torch.optim.AdamW>` , the second way to compute the update is directly recompute the update with AdamW optimizer states.

.. code-block:: python
    :emphasize-lines: 6,7

    # train loops start here
        # model forward
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        weight, grad, (m, v) = nnstat.from_together(model, optimizer, ["exp_avg", "exp_avg_sq"])
        update_adam = -lr * m / (torch.sqrt(v) + 1e-8)

This repo uses AdamW with epsilon 1e-8, and we ignore the bias correction and weight decay here. Now let's examine the embedding layer of adam update.

>>> weight
DistributedDataParallel_weights[L2=639, Numel=124,373,760, device=cuda:0, dtype=None]
(
        00: module.transformer.wte.weight              (50304, 768)
        01: module.transformer.wpe.weight               (1024, 768)
        02: module.transformer.h.0.ln_1.weight               (768,)
        (...truncated)
)
>>> torch.allclose(update.at(0), update_adam.at(0), atol=1e-7)
True
>>> torch.allclose(update.at(0), update_adam.at(0), atol=1e-8)
False

Since we are using fp32, the difference is acceptable (fp32 has 23 bits of mantissa, and thus the precision is about :math:`2^{-23}\approx 1\times 10^{-7}`).

Compute the Trust Ratio
----------------------------

Trust ratio measures the ratio of the weight norm to the update norm, defined in `Large Batch Training of Convolutional Networks <https://arxiv.org/abs/1708.03888>`_. We can use this metric to check if the updates among different layers are balanced. Here we define the trust ratio as the reciprocal of the trust ratio defined in the original paper since this definition is more intuitive.

The trust ratio can be computed in one line. There is also a function :func:`trust_ratio <nnstat.functional.trust_ratio>` to compute the trust ratio.

>>> trust_ratio = update.norm(2, lw=True) / weight.norm(2, lw=True)
>>> trust_ratio_mlp = nnstat.trust_ratio(update, weight, pattern='.*mlp.*')
>>> trust_ratio_mlp.describe(lw=True)
[===================== Stats StateDict =====================]
    no name                                        value
0    0  module.transformer.h.0.mlp.c_fc.weight     0.001064
1    1  module.transformer.h.0.mlp.c_proj.weight   0.001172
2    2  module.transformer.h.1.mlp.c_fc.weight     0.001089
3    3  module.transformer.h.1.mlp.c_proj.weight   0.000974
4    4  module.transformer.h.2.mlp.c_fc.weight     0.001014
5    5  module.transformer.h.2.mlp.c_proj.weight   0.001087
6    6  module.transformer.h.3.mlp.c_fc.weight     0.001020
7    7  module.transformer.h.3.mlp.c_proj.weight   0.001065
8    8  module.transformer.h.4.mlp.c_fc.weight     0.001040
9    9  module.transformer.h.4.mlp.c_proj.weight   0.001049
10  10  module.transformer.h.5.mlp.c_fc.weight     0.001067
11  11  module.transformer.h.5.mlp.c_proj.weight   0.001017
(...truncated)

As we can see, the trust ratios of the MLP layers are close to each other.
